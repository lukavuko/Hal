services:
  # Streamlit UI - User interface for monitoring and control
  interface:
    build: ./interface
    container_name: hal-interface
    ports:
      - "8501:8501"    # Streamlit UI
      - "5050:5050"    # Internal API
    volumes:
      - hal-data:/app/data
    environment:
      - BACKEND_URL=http://backend:5000
    depends_on:
      - backend
    networks:
      - hal-network
    restart: unless-stopped

  # Backend - Unified Vision, Mind, Speech services
  backend:
    build: ./backend
    container_name: hal-backend
    ports:
      - "5000:5000"
    volumes:
      - hal-data:/app/data
      - hal-models:/app/models
    # Essential for Windows -> Linux webcam passthrough
    devices:
      - /dev/video0:/dev/video0
    group_add:
      - video
    environment:
      - PORT=5000
      # Use host Ollama (running natively on Windows)
      - OLLAMA_URL=http://host.docker.internal:11434
      - PIPER_CACHE=/app/models
      - USE_CUDA=true
      - CONFIG_PATH=src/models.yml
    # develop:
    #   watch:
    #     - action: sync+restart
    networks:
      - hal-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Ollama - LLM inference server (optional if running natively)
  # Uncomment this service if you want Ollama in Docker instead of native
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: hal-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-models:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   networks:
  #     - hal-network
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #           - count: 1
  #             capabilities: [gpu]

volumes:
  hal-data:
    name: hal-data
  hal-models:
    name: hal-models
  ollama-models:
    name: ollama-models

networks:
  hal-network:
    name: hal-network
    driver: bridge
